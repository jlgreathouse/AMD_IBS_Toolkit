#!/usr/bin/env python
# coding=utf-8
#
# Copyright (C) 2017 Advanced Micro Devices, Inc.

# This file is distributed under the BSD license described in tools/LICENSE

"""IBS Run and Annotate Tools
This script will run an application and gather IBS op and/or fetch samples.
It will then take those samples and annotate them with information about
which instructions (and lines of code) were sampled.

It does this by first asking the IBS monitor application (found in
tools/ibs_monitor/) to save off dynamic library linkage when the
application-under-test is started. This is done by using glibc's
LD_DEBUG mechanism. This allows us to understand the base address and range
of any dynamic libraries; IBS samples with an instruction pointer into that
region of memory were from code in that dynamically linked library.

This tool will then automatically perform this mapping of IBS samples to
the x86 instruction (and line of code) that they represent, based on the
IBS samples' instruction pointer. This is done with the 'objdump' utilty.
Mapping IBS samples to instructions this way should almost always works;
mapping IBS samples to lines of code will require the target application
be built with with debug symbols.
"""

from __future__ import print_function
from multiprocessing import cpu_count, Process
from subprocess import Popen, PIPE
from operator import itemgetter
from bisect import bisect
from time import time
from tempfile import mkstemp
from shutil import copyfileobj
from distutils.spawn import find_executable
from textwrap import wrap
import csv
import os
import sys
import argparse
from joblib import Parallel, delayed

# RIP_DICT is global dictionary. However
# joblib.Parallel will fork #num_cores processes
# which will each have its own copy of global RIP_DICT
# We can't pass this into the job, because then the cache will
# never actually be filled.
RIP_DICT = {}
VERBOSE = 0
def vprint(msg):
    """Print to the screen if running this script in VERBOSE mode"""
    if VERBOSE:
        print(msg)

# Instruction lookup takes in a list of RIPs to look up.
# This assumes that the list of library base+limits have already been put
# into the global lib_base and lib_size Maps.
def inst_lookup(in_file, start_byte, end_byte, pid, poi, fetch=False):
    """Parallel worker thread that takes an input line from an IBS CSV file,
    looks up the instruction pointer for each sample, and tries to find where
    in the binary or libraries it was located (and which instruction it was).

    It will ignore samples not from the target process, any samples from kernel
    mode, and any fetch samples without valid physical addrsses.

    It uses objdump to obtain this informatino; because calling a separate
    process like objdump can be very slow, this function attempts to cache
    any lookups it does, and actually prefetches onto future instructions
    in an attempt to proactively fill up that cache.

    Keyword arguments:
    in_file -- The name of the input file we will be reading with this
               parallel worker thread.
    start_byte -- Within the input file, where should this worker start?
    end_byte -- Within this input file, where will this worker stop?
    pid -- The process ID of the process that is under analysis.
    poi -- Path to the program of interest
    fetch -- Are these Fetch samples? (Default: False)
    """

    input_csv = open(in_file, "r")
    input_csv.seek(start_byte,0)

    temp_file = mkstemp(suffix='.csv', prefix='ibs_merge_', text=True)
    fout_name = temp_file[1]
    fout = open(fout_name, 'w+')
    csv_row_num = 0
    total_bytes = 0
    print_me = []
    for line in input_csv:
        total_bytes += len(line)
        if (start_byte + total_bytes) > end_byte:
            break

        line_csv = line.split(",")

        # The first line is the header. Ignore that.
        if line_csv[0] == "TSC":
            continue

        csv_row_num += 1

        # We must strip out any kernel-mode IBS samples, as well as any samples
        # not from the PID we care about. For fetch addresses, if it doesn't
        # have valid physical instruction address, then we do not care.
        # Col 3 is PID
        # Col 4 is kernel mode
        # Col 5 is Phy_valid in fetch mode
        if (int(line_csv[3]) == int(pid) and
                int(line_csv[4]) == 0 and
                ((not fetch) or (int(line_csv[5]) == 1))):
            if fetch:
                rip = int(line_csv[6], 16)
            else:
                rip = int(line_csv[5], 16)
        else:
            continue

        # First, check to see if we have decoded this RIP before. If so, just
        # pull it from the cache. This will make the decoding run faster.
        real_line = line
        if rip in RIP_DICT:
            data_to_output = RIP_DICT[rip]
        else:
            # Check to see if this RIP was from a dynamically linked library.
            lib_idx = bisect(lib_base, rip) - 1
            if lib_idx < 0:
                # Was in the program itself.
                base_offset = 0
                inst_offset = rip
                obj_name = poi
            elif rip < lib_base[lib_idx] + lib_size[lib_idx]:
                # Was in a library.
                base_offset = lib_base[lib_idx]
                inst_offset = (rip - base_offset)
                obj_name = libs[lib_idx][2]
            else:
                # No idea where it came from
                if VERBOSE:
                    sys.stderr.write('RIP {} not found.\n'.format(hex(rip)))
                continue

            # We use objdump to decode these instructions.
            # Set width to 15, which is the longest legal AMD64 instruction.
            # We start by trying to decode the actual RIP (or decode the offset
            # of this RIP in the target library). We then try to decode another
            # kB of instructions afterwards, to prefetch into our cache.
            # We have a cache of decodes because calling objdump repeatedly for
            # every instruction can lead to huge slowdowns.
            cmd = ['objdump', '-dl', '--insn-width=15 '
                   , '--start-address=' + hex(inst_offset)         \
                   , '--stop-address=' + hex(inst_offset + 0x3ff)  \
                   , obj_name                                      \
                ]
            proc = Popen(cmd, stdout=PIPE, universal_newlines=True)
            out = proc.communicate()[0]
            inst_idx_beg = out.find(hex(inst_offset)[2:] + ':')
            # we tried objdump binary yet no inst found
            if inst_idx_beg < 0:
                if VERBOSE:
                    sys.stderr.write("RIP in dynsym or rela.dyn " + hex(rip) + "\n")
                continue
            inst_idx_end = out.find('\n', inst_idx_beg) + 1
            # dumping the above line that has source code info
            source_idx_end = out.rfind('\n', 0, inst_idx_beg)
            source_idx_beg = out.rfind('\n', 0, source_idx_end) + 1
            source_info = out[source_idx_beg:source_idx_end-1]
            inst = out[source_idx_end+1:inst_idx_end].lstrip().split('\t')
            inst[0] = "0x" + inst[0].rstrip(':')
            inst[1] = "0x" + inst[1].replace(' ', '')
            if len(inst) > 2:
                inst[2] = inst[2].lstrip().rstrip().replace(',', ' ')
                inst[2] += ',\n'
            else:
                inst.append(',\n')
            res = source_info + ',' + ','.join(inst)
            RIP_DICT[rip] = res
            data_to_output = res
            # Everything down here is to add as many lines as possible into
            # the translation cache. So we try to go through as many "extra"
            # translations as possible. This is why we set the end address
            # of the objdump as 0x3ff after our initial translation.
            following_lines = out[inst_idx_end:].splitlines()
            for line in following_lines:
                presumable_addr = line[:line.find(':')]
                if line.find("(bad)") != -1:
                    # Some of the future translations of objdump will be bad
                    # because objdump will assume that every future instruction
                    # is forwarded decoded from our start address. If this
                    # future instruction is not aligned with the initial
                    # address, then the translation can come back as (bad).
                    # Don't add that into the cache -- this is useless work.
                    # We shouldn't see these unaligned addresses in the future.
                    break
                try:
                    rip_in_block = base_offset + int(presumable_addr, 16)
                    inst = line.split('\t')
                    inst[0] = "0x" + inst[0].lstrip().rstrip(':')
                    inst[1] = "0x" + inst[1].replace(' ', '')
                    if len(inst) > 2:
                        inst[2] = inst[2].lstrip().rstrip().replace(',', ' ')
                        inst[2] += ',\n'
                    else:
                        # We should not see any lines with a "blank" decoding.
                        # This normally happens when the instruction is longer
                        # than the objdump insn-width. However, we should have
                        # set that to the maximum x86 instruction width.
                        # This might be a bad instruction, so don't cache it.
                        break
                    res = source_info + ',' + ','.join(inst)
                    RIP_DICT[rip_in_block] = res
                except ValueError:
                    source_info = line[:-1]
        print_me.append(real_line.strip() + data_to_output.strip() + "\n")
    fout.write(''.join(print_me))
    fout.close()
    input_csv.close()
    return fout_name


# dumpling the lib infos: path, base, size
def read_ld_debug(ld_debug_file):
    """This function reads information from a text file that was output by ld.so
    based on the LD_DEBUG environment variable. This will allow our tool to
    understand the path to any included libraries, the base address used during
    this run of the application, and the size in memory of that loaded library.

    Keyword arguments:
    ld_debug_file -- path to the text file with LD_DEBUG outputs.
    """
    lib_list = []
    with open(ld_debug_file, 'r') as fin:
        pid_from_file = fin.readline().split(':')[0].strip()
        lines = fin.readlines()
        for i, line in enumerate(lines):
            if 'base:' in line:
                tmp = line.split()
                lib_list.append((int(tmp[4], 16), int(tmp[6], 16),
                                 lines[i - 3].split('=')[1].strip()))
    # sort the libs based on their base addr
    lib_list.sort(key=itemgetter(0))
    vprint('Finished dumping library information.')
    return lib_list, pid_from_file

# Dump the CSV file into chunks of 4k rows so that we can process them
# in parallel. This generator will yield a [list of row_strings]
def dump_csv(samples_filename):
    """Open a CSV file and create lists out of the lines.
    This will yield multiple lists, each of which will have up to 4k lines
    from the file in it. This allows us to process these lines in parallel.
    """
    row_buf = []
    row_count = 0

    start_byte = 0
    end_byte = 0
    with open(samples_filename, 'r') as fin:
        for row in fin:
            end_byte += len(row)
            row_count = row_count + 1
            if row_count % 0x4000 == 0:
                yield start_byte, end_byte
                start_byte = end_byte
                row_count = 0
        if row_count != 0:
            yield start_byte, end_byte

def write_csv(all_temp_files, in_csv_file, out_file):
    """Write a final CSV file based on a large number of annotated IBS CSV
    files that were created in parallel. This will also create the new
    annotated header.
    """
    # We add four columns to the CSV's header
    with open(in_csv_file, 'r') as fin:
        first_line = fin.readline().split(',')
    fin.close()
    with open(out_file, 'w+') as fout:
        fout_csv = csv.writer(fout, lineterminator='\n')
        first_line.pop() # kill the trailing newline
        first_line.extend(['Source_Line', 'Binary_Offset', 'Opcode', 'Instruction'])
        fout_csv.writerow(first_line)
        for cat_file in all_temp_files:
            copyfileobj(open(cat_file, 'r'), fout)
            os.remove(cat_file)
        fout.close()

def parse_and_run_ibs():
    script =  sys.argv[0]
    """Parse the arguments for this script."""
    parser = argparse.ArgumentParser(description='\n'.join(wrap('This tool '
                                     'will run an application and gather IBS '
                                     'op and/or fetch samples from it. It will '
                                     'then take those samples and annotate '
                                     'them with information about which '
                                     'instructions (and lines of code) were '
                                     'sampled.',80)) + '\n\n' +
                                     '\n'.join(wrap('You should put a " -- " '
                                     'between the arguments to this tool and '
                                     'the command to run if it will have its '
                                     'own options. For example:',80)) + "\n" +
                                     str(script) + ' -o -- ls -alh\n\n' +
                                     '\n'.join(wrap('If you wish to redirect '
                                     'the output of your target command (and '
                                     'not this tool), place the command to run '
                                     'in double quotes and use your /bin/sh '
                                     'shell\'s syntax to do the redirect. For '
                                     'example:',80)) + '\n' + str(script) +
                                     ' -o -- "ls -alh > output 2>&1"\n\n',
                                     formatter_class=
                                     argparse.RawDescriptionHelpFormatter)
    parser.add_argument('-l', '--lookup', action='store_true',
                        dest='lookup_only',
                        help='Perform instruction lookup/parsing only. '\
                        'Skip running the application and use existing IBS '\
                        'traces instead. Will look for op_csv_name and '\
                        'fetch_csv_name as inputs.')
    parser.add_argument('-o', '--op_sample', action='store',
                        dest='op_sample_rate', const='262144', default='0',
                        nargs='?',
                        help='Gather/annotate IBS op samples. Without passing '\
                        'a number to this argument, we will default to '\
                        'sampling every 256K ops. Optionally pass a number '\
                        'to change the op sampling rate.')
    parser.add_argument('-f', '--fetch_sample', action='store',
                        dest='fetch_sample_rate', const='65536', default='0',
                        nargs='?',
                        help='Gather/annotate IBS fetch samples. Without '\
                        'passing a number to this argument, we will default '\
                        'to sampling every 64K fetches. Optionally pass a '
                        'number to change fetch sampling rate.')
    parser.add_argument('-t', '--temp_dir',
                        default=os.path.abspath(os.getcwd()),
                        help='Directory used to store the temporary IBS '\
                        'trace files. This includes the raw IBS data '\
                        'output from the monitor and the CSV files output '\
                        'from the IBS decoder.')
    parser.add_argument('--op_samples', default='ibs_op.dat',
                        help='File used to store IBS op traces from the '\
                        'monitor. This file will be stored into the TEMP_DIR '\
                        'directory.')
    parser.add_argument('--fetch_samples', default='ibs_fetch.dat',
                        help='File used to store IBS fetch traces from the '\
                        'monitor. This file will be stored into the TEMP_DIR '\
                        'directory.')
    parser.add_argument('--op_csv', default='ibs_op.csv',
                        help='File used to hold decoded IBS op traces. '\
                        'Will create/look for this file in the TEMP_DIR '\
                        'directory.')
    parser.add_argument('--fetch_csv', default='ibs_fetch.csv',
                        help='File used to hold decoded IBS fetch traces. '\
                        'Will create/look for this file in the TEMP_DIR '\
                        'directory.')
    parser.add_argument('--ld_debug_file', default='ld_debug.txt',
                        help='File to hold the LD_DEBUG information about '\
                        'the application, so that we can understand its '\
                        'dynamic library locations. Will create/look for '\
                        'this file in the TEMP_DIR directory.')
    parser.add_argument('-d', '--out_dir',
                        default=os.path.abspath(os.getcwd()),
                        help='Directory used to store the tool output.')
    parser.add_argument('--op_output', default='ibs_annotated_op.csv',
                        help='File used to hold the annotated IBS op traces. '\
                        'Will be created in the OUT_DIR directory.')
    parser.add_argument('--fetch_output', default='ibs_annotated_fetch.csv',
                        help='File used to hold the annotated IBS fetch '\
                        'traces. Will be created in the OUT_DIR directory.')
    parser.add_argument('-w', '--working_dir',
                        default=os.path.abspath(os.getcwd()),
                        help='Set the working directory for the program '\
                        'under test.')
    parser.add_argument('--timer', action='store_true',
                        help='Time different sections of this runscript.')
    parser.add_argument('command', nargs="*",
                        help='Command to run program under test, or pointer '\
                        'to the binary that was used to gather the CSV '\
                        'files. This will be combined with WORKING_DIR if '\
                        'the command is not an absolute path. If WORKING_DIR '\
                        'is not set, or the binary is not found in that '\
                        'directory, this tool will check the PATH '\
                        'environment variable.')
    args = parser.parse_args()

    if not (args.op_sample_rate != '0' or args.fetch_sample_rate != '0'):
        print("Error! Neither IBS op nor IBS fetch samples were requested.")
        parser.error('Select -o and/or -f to work on op and/or fetch samples')

    # After parsing all the arguments, set up directory and file paths
    ibs_tools_dir = os.path.join(os.path.dirname(__file__), "..")
    op_sample_file = os.path.join(args.temp_dir, args.op_samples)
    fetch_sample_file = os.path.join(args.temp_dir, args.fetch_samples)
    op_csv_file = os.path.join(args.temp_dir, args.op_csv)
    fetch_csv_file = os.path.join(args.temp_dir, args.fetch_csv)
    ld_debug_file = os.path.join(args.temp_dir, args.ld_debug_file)
    op_out_file = os.path.join(args.out_dir, args.op_output)
    fetch_out_file = os.path.join(args.out_dir, args.fetch_output)

    # The final thing parsed out is the application to run (and the rest
    # of *its* arguments, optionally). Let's find the application and make
    # a variable that holds its path. If the command line passed in is not
    # an absolute path (or something like ~/foo.exe), then we try to find it
    # in the directory pointed to by the --working_dir argument. If it's
    # also not there, we fall back to checking the PATH environment vars.
    prog = args.command

    # If prog was passed in with double quotes, then prog[0] is a string
    if len(prog) == 1:
        # split the string on spaces so we can see what the app actually was
        prog_unquote = prog[0].split(' ')
        prog = prog_unquote

    prog[0] = os.path.expanduser(prog[0])
    prog[0] = os.path.expandvars(prog[0])
    if os.path.isabs(prog[0]):
        bench_file = prog[0]
    else:
        bench_file = find_executable(prog[0])
        if bench_file == None:
            bench_file = os.path.join(args.working_dir, prog[0])

    if not os.path.isfile(bench_file):
        print("Error! Cannot find the desired program: " + prog[0])
        sys.exit("Will not be able to properly gather/decode the IBS trace data.")

    if not args.lookup_only:
        # Set up the command line to run the IBS monitoring application
        ibs_monitor_bin = os.path.join(ibs_tools_dir, 'ibs_monitor/ibs_monitor')
        if not os.path.isfile(ibs_monitor_bin):
            print("Error! The IBS monitor application was not found.")
            print("Have you run 'make' in the tools directory?")
            print("    " + str(ibs_tools_dir))
            sys.exit("Could not run requested commands.")
        ibs_monitor_cmd = [ibs_monitor_bin, '-l', ld_debug_file]
        if args.working_dir:
            ibs_monitor_cmd += ['-w', args.working_dir]
        if args.op_sample_rate != '0':
            ibs_monitor_cmd += ['-o', op_sample_file]
            ibs_monitor_cmd += ['-r', args.op_sample_rate]
        if args.fetch_sample_rate != '0':
            ibs_monitor_cmd += ['-f', fetch_sample_file]
            ibs_monitor_cmd += ['-s', args.fetch_sample_rate]
        ibs_monitor_cmd += prog

        # Set up the command to run the IBS decoding application
        ibs_decoder_bin = os.path.join(ibs_tools_dir, 'ibs_decoder/ibs_decoder')
        if not os.path.isfile(ibs_monitor_bin):
            print("Error! The IBS monitor application was not found.")
            print("Have you run 'make' in the tools directory?")
            print("    " + str(ibs_tools_dir))
            sys.exit("Could not run requested commands.")
        ibs_decoder_cmd = [ibs_decoder_bin]
        if args.op_sample_rate != '0':
            ibs_decoder_cmd += ['-i', op_sample_file]
            ibs_decoder_cmd += ['-o', op_csv_file]
        if args.fetch_sample_rate != '0':
            ibs_decoder_cmd += ['-f', fetch_sample_file]
            ibs_decoder_cmd += ['-g', fetch_csv_file]

        # run IBS monitor and decoder with program of interest
        print('IBS Tools Running Command: ' + ' '.join(prog))
        print('    From directory: ' + str(args.working_dir))
        try:
            ret_val = Popen(' '.join(ibs_monitor_cmd), shell=True).wait()
            if ret_val != 0:
                raise ValueError('Failed to run the IBS monitor!')
            ret_val = Popen(' '.join(ibs_decoder_cmd), shell=True).wait()
            if ret_val != 0:
                raise ValueError('Failed to run the IBS decoder!')
        except OSError:
            parser.error('IBS Tools not found!')
        except ValueError as err:
            sys.exit(str(err.args[0]))
    else:
        # check if sample csv files exist
        if args.op_sample_rate != '0' and not os.path.exists(op_csv_file):
            parser.error('IBS op samples specified, but CSV file '
                         + op_csv_file + ' not found!')
        if args.fetch_sample_rate != '0' and not os.path.exists(fetch_csv_file):
            parser.error('IBS fetch samples specified, but CSV file '
                         + fetch_csv_file + ' not found!')
        if not os.path.exists(ld_debug_file):
            parser.error('IBS LD_DEBUG info file ('
                         + ld_debug_file + ') does not exist!')
    return (bench_file, args.op_sample_rate, args.fetch_sample_rate,
            op_csv_file, fetch_csv_file, ld_debug_file, op_out_file,
            fetch_out_file, args.timer)

def main():
    """Main function for this application"""
    app_start_time = time()

    # poi: program of interest
    (poi, dump_op_rate, dump_ft_rate, op_csv_fn, fetch_csv_fn, ld_debug_fn,
     op_out_fn, fetch_out_fn, timer) = parse_and_run_ibs()

    annotate_start_time = time()

    # Start by gathering information about the libraries that were dynamically
    # linked when the application ran.
    # Putting these into globals so that we don't have to push them through IPC
    # to the parallel processes we will launch with joblib.
    global libs
    global lib_base
    global lib_size
    libs, pid_to_use = read_ld_debug(ld_debug_fn)
    vprint('Dumping info for process {}'.format(pid_to_use))
    lib_base = map(itemgetter(0), libs)
    lib_size = map(itemgetter(1), libs)

    # Next, split off parallel jobs to look up the IBS fetch or op samples
    # within the application or its shared libraries.
    num_cores = cpu_count()
    vprint('Running IBS lookups in parallel with {} jobs'.format(num_cores))

    dump_and_lookup_start_time = time()

    if dump_op_rate != '0':
        print("Beginning looking up instructions for IBS op samples...")
        op_lookup_start_time = time()
        op_res = Parallel(n_jobs=num_cores-1, pre_dispatch=('1.5*n_jobs'))(
            delayed(inst_lookup)(op_csv_fn, start_byte, end_byte, pid_to_use, poi, False)
            for start_byte, end_byte in dump_csv(op_csv_fn))
        op_lookup_done_time = time()
        print("Done with IBS op lookups.")
    if dump_ft_rate != '0':
        proc = None
        if dump_op_rate != '0':
            # pipeline the final file write with fetch sample processing
            proc = Process(target=write_csv,
                           args=(op_res, op_csv_fn, op_out_fn))
            proc.start()
        print("Beginning looking up instructions for IBS fetch samples...")
        fetch_lookup_start_time = time()
        ft_res = Parallel(n_jobs=num_cores-1, pre_dispatch=('1.5*n_jobs'))(
            delayed(inst_lookup)(fetch_csv_fn, start_byte, end_byte, pid_to_use, poi, True)
            for start_byte, end_byte in dump_csv(fetch_csv_fn))
        fetch_lookup_done_time = time()
        print("Done with IBS fetch lookups.")
        write_csv(ft_res, fetch_csv_fn, fetch_out_fn)
        fetch_write_done_time = time()
        if proc is not None:
            proc.join()
    elif dump_op_rate != '0':
        write_csv(op_res, op_csv_fn, op_out_fn)
        op_write_done_time = time()

    # If the user has requested timing information, print out all of the
    # timers from across the application.
    if timer:
        end_time = time()
        total_time = end_time - app_start_time
        app_runtime = annotate_start_time - app_start_time
        library_parse = dump_and_lookup_start_time - annotate_start_time
        lookup_and_write = end_time - dump_and_lookup_start_time
        print("IBS run and annotate time: %.3f s" % total_time)
        print("\tApplication time: %.3f s" % app_runtime)
        print("\tParsing libraries time: %.3f s" % library_parse)
        print("\tAnnotate and CSV write time: %.3f s" % lookup_and_write)
        if dump_op_rate != '0':
            op_lookup = op_lookup_done_time - op_lookup_start_time
            print("\t\tIBS Op lookup time: %.3f s" % op_lookup)
            # The op write timer only exists if we are not doing fetch lookups.
            # Otherwise, the fetch lookups and writing take place in parallel.
            if dump_ft_rate == '0':
                op_write = op_write_done_time - op_lookup_done_time
                print("\t\tIBS Op CSV write time: %.3f s" % op_write)
        if dump_ft_rate != '0':
            fetch_lookup = fetch_lookup_done_time - fetch_lookup_start_time
            fetch_write = fetch_write_done_time - fetch_lookup_done_time
            if dump_op_rate != '0':
                possible_op_write = (lookup_and_write - fetch_lookup -
                                     fetch_write - op_lookup)
                if possible_op_write > 0.5:
                    print("\t\tIBS Op CSV write time: %.3f s" %
                          possible_op_write)
                else:
                    print("\t\tIBS Op CSV write time: 0.000 s (fully pipelined)")
            print("\t\tIBS Fetch lookup time: %.3f s" % fetch_lookup)
            print("\t\tIBS Fetch CSV write time: %.3f s" % fetch_write)

if __name__ == '__main__':
    main()
